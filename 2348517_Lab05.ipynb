{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIulWcBtHQF1xFm9LC+dg4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Epsita-R/NLP/blob/main/2348517_Lab05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antonyms from WordNet."
      ],
      "metadata": {
        "id": "lYDH9JNL4mi7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyFrZ8ik4JAq",
        "outputId": "a11561ed-c65c-4c7c-ed1e-388bf8a7a8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms of 'good': bad, badness, evilness, evil, ill\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download WordNet data\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_antonyms(word):\n",
        "    antonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            for antonym in lemma.antonyms():\n",
        "                antonyms.add(antonym.name())\n",
        "    return antonyms\n",
        "word = \"good\"\n",
        "antonyms = get_antonyms(word)\n",
        "print(f\"Antonyms of '{word}': {', '.join(antonyms)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming non-English words."
      ],
      "metadata": {
        "id": "S5VEKlve46hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def stem_non_english_word(word, language_code):\n",
        "    stemmer = SnowballStemmer(language_code)\n",
        "    return stemmer.stem(word)\n",
        "\n",
        "word = \"amigos\"\n",
        "language_code = \"spanish\"\n",
        "\n",
        "stemmed_word = stem_non_english_word(word, language_code)\n",
        "print(f\"Stemmed word in {language_code}: {stemmed_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKVfVigK5FcJ",
        "outputId": "e6209245-6ecc-4955-cfef-a32e1b7f231d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word in spanish: amig\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatizing words Using WordNet"
      ],
      "metadata": {
        "id": "MaQ58cbP52KD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_and_stem(word):\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, wordnet.NOUN)\n",
        "    snowball_stem = snowball_stemmer.stem(word)\n",
        "    porter_stem = porter_stemmer.stem(word)\n",
        "    lancaster_stem = lancaster_stemmer.stem(word)\n",
        "\n",
        "    return lemmatized_word, snowball_stem, porter_stem, lancaster_stem\n",
        "\n",
        "word = \"running\"\n",
        "lemmatized, snowball_stem, porter_stem, lancaster_stem = lemmatize_and_stem(word)\n",
        "print(f\"Word: {word}\")\n",
        "print(f\"Lemmatized Word: {lemmatized}\")\n",
        "print(f\"Snowball Stem: {snowball_stem}\")\n",
        "print(f\"Porter Stem: {porter_stem}\")\n",
        "print(f\"Lancaster Stem: {lancaster_stem}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHib-eXo56XO",
        "outputId": "56ac18ec-cb33-448f-d93a-1949726cb9b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: running\n",
            "Lemmatized Word: running\n",
            "Snowball Stem: run\n",
            "Porter Stem: run\n",
            "Lancaster Stem: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differentiate stemming and lemmatizing words."
      ],
      "metadata": {
        "id": "hAzwc5Y26Ul7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def stem_vs_lemmatize(word):\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word)\n",
        "    return stemmed_word, lemmatized_word\n",
        "word_to_process = \"running\"\n",
        "stemmed, lemmatized = stem_vs_lemmatize(word_to_process)\n",
        "print(f\"Word: {word_to_process}\")\n",
        "print(f\"Stemmed Word: {stemmed}\")\n",
        "print(f\"Lemmatized Word: {lemmatized}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5-wN1cH6ZGQ",
        "outputId": "ec278a6f-7ebe-4b32-9914-1f200e27fe17"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: running\n",
            "Stemmed Word: run\n",
            "Lemmatized Word: running\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " PoS Tagging"
      ],
      "metadata": {
        "id": "QtqoP6tp6xHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "text = \"I am Epsita Rajesh. I am pursuing masters in Artificial Intelligence and Machine Learning from Christ University. I am from Bangalore.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged_words = pos_tag(tokens)\n",
        "\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ipfSxBQ6yUB",
        "outputId": "49289935-d254-445c-b17c-b7301372a4ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('Epsita', 'JJ'), ('Rajesh', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('pursuing', 'VBG'), ('masters', 'NNS'), ('in', 'IN'), ('Artificial', 'NNP'), ('Intelligence', 'NNP'), ('and', 'CC'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('from', 'IN'), ('Christ', 'NNP'), ('University', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Bangalore', 'NNP'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the Named Entity Recognition."
      ],
      "metadata": {
        "id": "45LhnD_p7a2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('words')\n",
        "\n",
        "text = \"I am Epsita Rajesh. I am pursuing masters in Artificial Intelligence and Machine Learning from Christ University. I am from Bangalore.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tagged_words = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tagged_words)\n",
        "\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4J9W0OE7lj5",
        "outputId": "8fc19fad-42c0-4e4e-dadb-2f6fab86a4f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  I/PRP\n",
            "  am/VBP\n",
            "  (PERSON Epsita/JJ Rajesh/NNP)\n",
            "  ./.\n",
            "  I/PRP\n",
            "  am/VBP\n",
            "  pursuing/VBG\n",
            "  masters/NNS\n",
            "  in/IN\n",
            "  (ORGANIZATION Artificial/NNP Intelligence/NNP)\n",
            "  and/CC\n",
            "  (PERSON Machine/NNP Learning/NNP)\n",
            "  from/IN\n",
            "  (ORGANIZATION Christ/NNP University/NNP)\n",
            "  ./.\n",
            "  I/PRP\n",
            "  am/VBP\n",
            "  from/IN\n",
            "  (GPE Bangalore/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency Parsing and Constituency Parsing."
      ],
      "metadata": {
        "id": "0dSvmukn8C4w"
      }
    }
  ]
}